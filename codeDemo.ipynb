{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codeDemo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Discovering and Categorising Languages Biases in Reddit"
      ],
      "metadata": {
        "id": "rPTLoinBCGJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Abstract \n",
        "This paper utilizes word embeddings to automatically discover and categorise biases in different Reddit communities. The authors developed a method to automatically discover and categorize protected attributes in different subreddits. "
      ],
      "metadata": {
        "id": "fiFUAyYuCZm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Basic Approach\n",
        "Given two sets of concepts (c1 = {he, son, his, him, father, male}, and c2 = {she, daughter, her, mother, female}) and a word embedding model, the approach is as follows:\n",
        "\n",
        "1.Train an embedding model on corpus and select two sets of target words\n",
        "\n",
        "2.Select n-most biased words with respect to each concept\n",
        "\n",
        "3.Cluster words into k-conceptual biases\n",
        "\n",
        "4.Categorize the discovered biases\n"
      ],
      "metadata": {
        "id": "5Zgx5-CgDM-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgNc7WGIeZxB",
        "outputId": "a30cfcc7-17d7-496e-88f0-357186fe5329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        " # Import necessary packages\n",
        "import pandas as pd\n",
        "import gensim \n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import nltk\n",
        "import time\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from scipy import spatial\n",
        "from sklearn.cluster import KMeans\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Dataset \n",
        "For the demo purppose, we will use a toy dataset. The dataset contains around 1,000,000 data. "
      ],
      "metadata": {
        "id": "U2El6x5DGOa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "H6BybAdKefY0",
        "outputId": "7375d6a0-899b-4449-901b-d6248358d738"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4db537f-b917-40c3-b64c-5817ebb931b5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4db537f-b917-40c3-b64c-5817ebb931b5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving toy_1000_trp.csv to toy_1000_trp (1).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"toy_1000_trp.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "OTfQQ0D1EFmH",
        "outputId": "65fb0a4e-c027-40d7-ead1-b6235d22a42b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (1,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3c015362-ac67-4640-bb1e-24f158e48199\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idint</th>\n",
              "      <th>idstr</th>\n",
              "      <th>created</th>\n",
              "      <th>author</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.620441e+10</td>\n",
              "      <td>t1_c1dfjia</td>\n",
              "      <td>1.295488e+09</td>\n",
              "      <td>highpowered</td>\n",
              "      <td>First? - Anyway, no that is not normal, and it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.620442e+10</td>\n",
              "      <td>t1_c1dfo0d</td>\n",
              "      <td>1.295490e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>&gt; He constantly speaks about his first love an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.620442e+10</td>\n",
              "      <td>t1_c1dfo3c</td>\n",
              "      <td>1.295490e+09</td>\n",
              "      <td>bluewasabi</td>\n",
              "      <td>I don't know how long you and your boyfriend h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.620442e+10</td>\n",
              "      <td>t1_c1dfpem</td>\n",
              "      <td>1.295491e+09</td>\n",
              "      <td>IOnlyReadPostTitles</td>\n",
              "      <td>&gt; BF lives in the past...I would like to move ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.620442e+10</td>\n",
              "      <td>t1_c1dfpld</td>\n",
              "      <td>1.295491e+09</td>\n",
              "      <td>[deleted]</td>\n",
              "      <td>It's normal to retain feelings for past loves,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c015362-ac67-4640-bb1e-24f158e48199')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c015362-ac67-4640-bb1e-24f158e48199 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c015362-ac67-4640-bb1e-24f158e48199');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          idint  ...                                               body\n",
              "0  2.620441e+10  ...  First? - Anyway, no that is not normal, and it...\n",
              "1  2.620442e+10  ...  > He constantly speaks about his first love an...\n",
              "2  2.620442e+10  ...  I don't know how long you and your boyfriend h...\n",
              "3  2.620442e+10  ...  > BF lives in the past...I would like to move ...\n",
              "4  2.620442e+10  ...  It's normal to retain feelings for past loves,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvIEqVNvEyfA",
        "outputId": "1349c2ce-7b16-477d-9ca9-6e211452e6be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999999"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Training the embedding model on toy_1000_trp.csv using word2vec"
      ],
      "metadata": {
        "id": "WwgWelhDF_IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainModel(csv_document, csv_comment_column='body', outputname='outputModel', window = 4, minf=10, epochs=100, ndim=200, lemmatiseFirst = False, verbose = True):\n",
        "\t'''\n",
        "\tLoad the documents from csv_document and column csv_comment_column, trains a skipgram embedding model with given parameters and saves it in outputname.\n",
        "\tcsv_document <str> : path to reddit csv dataset\n",
        "\tcsv_comment_column <str> : column where comments are stored\n",
        "\toutputname <str> : output model name\n",
        "\twindow = 4, minf=10, epochs=100, ndim=200, lemmatiseFirst = False, tolower= True : Training and preprocessing parameters\n",
        "\t'''\n",
        "\n",
        "\tdef loadCSVAndPreprocess(path, column = 'body', nrowss=None, verbose = True):\n",
        "\t\t'''\n",
        "\t\tinput:\n",
        "\t\tpath <str> : path to csv file\n",
        "\t\tcolumn <str> : column with text\n",
        "\t\tnrowss <int> : number of rows to process, leave None if all\n",
        "\t\tverbose <True/False> : verbose output\n",
        "\t\ttolower <True/False> : transform all text to lowercase\n",
        "\t\treturns:\n",
        "\t\tlist of preprocessed sentences\n",
        "\t\t'''\n",
        "\t\ttrpCom = pd.read_csv(path, lineterminator='\\n', nrows=nrowss)\n",
        "\t\tdocuments = []\n",
        "\t\tfor i, row in enumerate(trpCom[column]):\n",
        "\t\t\t\n",
        "\n",
        "\t\t\tif i%500000 == 0 and verbose == True:\n",
        "\t\t\t\tprint('\\t...processing line {}'.format(i))\n",
        "\t\t\ttry:\n",
        "\t\t\t\tpp = gensim.utils.simple_preprocess (row)\n",
        "\t\t\t\tif(lemmatiseFirst == True):\n",
        "\t\t\t\t\tpp = [wordnet_lemmatizer.lemmatize(w, pos=\"n\") for w in pp]\n",
        "\t\t\t\tdocuments.append(pp)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tif(verbose):\n",
        "\t\t\t\t\tprint('\\terror with row {}'.format(row))\n",
        "\t\tprint('Done reading all documents')\n",
        "\t\treturn documents\n",
        "\n",
        "\tdef trainWEModel(documents, outputfile, ndim, window, minfreq, epochss):\n",
        "\t\t'''\n",
        "\t\tdocuments list<str> : List of texts preprocessed\n",
        "\t\toutputfile <str> : final file will be saved in this path\n",
        "\t\tndim <int> : embedding dimensions\n",
        "\t\twindow <int> : window when training the model\n",
        "\t\tminfreq <int> : minimum frequency, words with less freq will be discarded\n",
        "\t\tepochss <int> : training epochs\n",
        "\t\t'''\n",
        "\t\tstarttime = time.time()\n",
        "\t\tprint('->->Starting training model {} with dimensions:{}, minf:{}, epochs:{}'.format(outputfile,ndim, minfreq, epochss))\n",
        "\t\tmodel = gensim.models.Word2Vec (documents, size=ndim, window=window, min_count=minfreq, workers=5)\n",
        "\t\tmodel.train(documents,total_examples=len(documents),epochs=epochss)\n",
        "\t\tmodel.save(outputfile)\n",
        "\t\tprint('->-> Model saved in {}'.format(outputfile))     \n",
        "\n",
        "     \n",
        "\tprint('->Starting with {} [{}], output {}, window {}, minf {}, epochs {}, ndim {}'.format(csv_document,csv_comment_column,outputname, window, minf, epochs, ndim))\n",
        "\tdocs = loadCSVAndPreprocess(csv_document, csv_comment_column, nrowss=None, verbose=verbose)\n",
        "\tstarttime = time.time()\n",
        "\tprint('-> Output will be saved in {}'.format(outputname))\n",
        "\ttrainWEModel(docs, outputname, ndim, window, minf, epochs)\n",
        "\tprint('-> Model creation ended in {} seconds'.format(time.time()-starttime))"
      ],
      "metadata": {
        "id": "KofO_dfkepBE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 Get the top most biased words"
      ],
      "metadata": {
        "id": "SGsG3VVtHc_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias($w$,$c_1$,$c_2$) = cos($\\vec{w},\\vec{c_1}$)-cos($\\vec{w},\\vec{c_2}$)"
      ],
      "metadata": {
        "id": "DZbZabvhJ_1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$c_1$,$c_2$ are the same as defined previously. To calculate the bias, we compute the cosine distance between the word and centroid of each target group. Word bias is treated on whether the similarity is higher for the first cos term or second (e.g. positive or negative values)."
      ],
      "metadata": {
        "id": "VBH1S2_bLJyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "def GetTopMostBiasedWords(modelpath, topk, c1, c2, pos = ['JJ','JJR','JJS'], verbose = True):\n",
        "\t'''\n",
        "\tmodelpath <str> : path to skipgram w2v model\n",
        "\ttopk <int> : topk words\n",
        "\tc1 list<str> : list of words for target set 1\n",
        "\tc2 list<str> : list of words for target set 2\n",
        "\tpos list<str> : List of parts of speech we are interested in analysing\n",
        "\tverbose <bool> : True/False\n",
        "\t'''\n",
        "\n",
        "\tdef calculateCentroid(model, words):\n",
        "\t\tembeddings = [np.array(model[w]) for w in words if w in model]\n",
        "\t\tcentroid = np.zeros(len(embeddings[0]))\n",
        "\t\tfor e in embeddings:\n",
        "\t\t\tcentroid += e\n",
        "\t\treturn centroid/len(embeddings)\n",
        "\n",
        "\tdef getCosineDistance(embedding1, embedding2):       \n",
        "\t\treturn spatial.distance.cosine(embedding1, embedding2)\n",
        "\n",
        "\n",
        "\t#select the interesting subset of words based on pos\n",
        "\tmodel = Word2Vec.load(modelpath)\n",
        "\twords_sorted = sorted( [(k,v.index, v.count) for (k,v) in model.wv.vocab.items()] ,  key=lambda x: x[1], reverse=False)\n",
        "\twords = [w for w in words_sorted if nltk.pos_tag([w[0]])[0][1] in pos]\n",
        "\n",
        "\tif len(c1) < 1 or len(c2) < 1 or len(words) < 1:\n",
        "\t\tprint('[!] Not enough word concepts to perform the experiment')\n",
        "\t\treturn None\n",
        "\n",
        "\tcentroid1, centroid2 = calculateCentroid(model, c1),calculateCentroid(model, c2)\n",
        "\twinfo = []\n",
        "\tfor i, w in enumerate(words):\n",
        "\t\tword = w[0]\n",
        "\t\tfreq = w[2]\n",
        "\t\trank = w[1]\n",
        "\t\tpos = nltk.pos_tag([word])[0][1]\n",
        "\t\twv = model[word]\n",
        "\t\tsent = sid.polarity_scores(word)['compound']\n",
        "\t\t#estimate cosinedistance diff\n",
        "\t\td1 = getCosineDistance(centroid1, wv)\n",
        "\t\td2 = getCosineDistance(centroid2, wv)\n",
        "\t\tbias = d2-d1\n",
        "\n",
        "\t\twinfo.append({'word':word, 'bias':bias, 'freq':freq, 'pos':pos, 'wv':wv, 'rank':rank, 'sent':sent} )\n",
        "\n",
        "\t\tif(i%100 == 0 and verbose == True):\n",
        "\t\t\tprint('...'+str(i), end=\"\")\n",
        "\n",
        "\t#Get max and min topk biased words...\n",
        "\tbiasc1 = sorted( winfo, key=lambda x:x['bias'], reverse=True )[:min(len(winfo), topk)]\n",
        "\tbiasc2 = sorted( winfo, key=lambda x:x['bias'], reverse=False )[:min(len(winfo), topk)]\n",
        "    #move the ts2 bias to the positive space\n",
        "\tfor w2 in biasc2:\n",
        "\t\tw2['bias'] = w2['bias']*-1\n",
        "    \n",
        "\treturn [biasc1, biasc2]"
      ],
      "metadata": {
        "id": "uxarVz3TfIko"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Clustering the bias words into two concepts"
      ],
      "metadata": {
        "id": "49SJexELIDym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Cluster(biasc1, biasc2, r, repeatk, verbose = True):\n",
        "\t'''\n",
        "\tbiasc1 list<words> : List of words biased towards target concept1 as returned by GetTopMostBiasedWords\n",
        "\tbiasc2 list<words> : List of words biased towards target concept2 as returned by GetTopMostBiasedWords\n",
        "\tr <int> : reduction factor used to determine k for the kmeans; k = r * len(voc) \n",
        "\trepeatk <int> : Number of Clustering to perform only to keep the partition with best intrasim\n",
        "\t'''\n",
        "\tdef getCosineDistance(embedding1, embedding2): \n",
        "\t\treturn spatial.distance.cosine(embedding1, embedding2)\n",
        "\tdef getIntraSim(partition):\n",
        "\t\tiS = 0\n",
        "\t\tfor cluster in partition:\n",
        "\t\t\tiS += getIntraSimCluster(cluster)\n",
        "\t\treturn iS/len(partition)\n",
        "\tdef getIntraSimCluster(cluster):\n",
        "\t\tif(len(cluster)==1):\n",
        "\t\t\treturn 0\n",
        "\t\tsim = 0; c = 0\n",
        "\t\tfor i in range(len(cluster)):\n",
        "\t\t\tw1 = cluster[i]['wv']\n",
        "\t\t\tfor j in range(i+1, len(cluster)):\n",
        "\t\t\t\tw2 = cluster[j]['wv']\n",
        "\t\t\t\tsim+= 1-getCosineDistance(w1,w2)\n",
        "\t\t\t\tc+=1\n",
        "\t\treturn sim/c\n",
        "\tdef createPartition(embeddings, biasw, k):\n",
        "\t\tpreds = KMeans (n_clusters=k).fit_predict(embeddings)\n",
        "\t\t#first create the proper clusters, then estiamte avg intra sim\n",
        "\t\tall_clusters = []\n",
        "\t\tfor i in range(0, k):\n",
        "\t\t\tclust = []\n",
        "\t\t\tindexes = np.where(preds == i)[0]\n",
        "\t\t\tfor idx in indexes:\n",
        "\t\t\t\tclust.append(biasw[idx])\n",
        "\t\t\tall_clusters.append(clust)\n",
        "\t\tscore = getIntraSim(all_clusters)\n",
        "\t\treturn [score, all_clusters]\n",
        "\n",
        "\n",
        "\tk = int(r * (len(biasc1)+len(biasc2))/2)\n",
        "\temb1, emb2  = [w['wv'] for w in biasc1], [w['wv'] for w in biasc2]\n",
        "\tmis1, mis2 = [0,[]], [0,[]]\t#here we will save partitions with max sim for both target sets\n",
        "\tfor run in range(repeatk):\n",
        "\t\tp1 = createPartition(emb1, biasc1, k)\n",
        "\t\tif(p1[0] > mis1[0]):\n",
        "\t\t\tmis1 = p1\n",
        "\t\tp2 = createPartition(emb2, biasc2, k)\n",
        "\t\tif(p2[0] > mis2[0]):\n",
        "\t\t\tmis2 = p2\n",
        "\t\tif(verbose == True):\n",
        "\t\t\tprint('New partition for ts1, intrasim: ', p1[0])\n",
        "\t\t\tprint('New partition for ts2, intrasim: ', p2[0])\n",
        "\n",
        "\tprint('[*] Intrasim of best partition found for ts1, ', mis1[0])\n",
        "\tprint('[*] Intrasim of best partition found for ts2, ', mis2[0])\n",
        "\treturn [mis1[1], mis2[1]]\n",
        "\t\t"
      ],
      "metadata": {
        "id": "q5iPajfhfMtA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Demo"
      ],
      "metadata": {
        "id": "cO0mKWatIT1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Train an embeddings model using word2vec with different parameters.\n",
        "'''\n",
        "setup = {'csvfile': \"toy_1000_trp.csv\", 'outputFile': 'Models', 'w':4, 'minf': 10, 'epochs':10 ,'ndim':200}\n",
        "    \n",
        "TrainModel(setup['csvfile'], \n",
        "           'body',\n",
        "           outputname = setup['outputFile'],\n",
        "           window = setup['w'],\n",
        "           minf = setup['minf'],\n",
        "           epochs = setup['epochs'],\n",
        "           ndim = setup['ndim'],\n",
        "           verbose = False\n",
        "           )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtsxJp55fkf1",
        "outputId": "9a7e6e6a-78b4-4e06-970b-cdde2d04bcf0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "->Starting with toy_1000_trp (1).csv [body], output Models, window 4, minf 10, epochs 10, ndim 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: DtypeWarning: Columns (1,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done reading all documents\n",
            "-> Output will be saved in Models\n",
            "->->Starting training model Models with dimensions:200, minf:10, epochs:10\n",
            "->-> Model saved in Models\n",
            "-> Model creation ended in 224.60853600502014 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "List of target sets used in this work, replace them in GetTopMostBiasedWords to obtain different sets of biases\n",
        "or create your own target sets to represent a concept!\n",
        "'''\n",
        "\n",
        "women=[\"sister\" , \"female\" , \"woman\" , \"girl\" , \"daughter\" , \"she\" , \"hers\" , \"her\"]\n",
        "men=[\"brother\" , \"male\" , \"man\" , \"boy\" , \"son\" , \"he\" , \"his\" , \"him\"]  \n",
        "\n",
        "islam = [\"allah\", \"ramadan\", \"turban\", \"emir\", \"salaam\", \"sunni\", \"koran\", \"imam\", \"sultan\", \"prophet\", \"veil\", \"ayatollah\", \"shiite\", \"mosque\", \"islam\", \"sheik\", \"muslim\", \"muhammad\"]\n",
        "christian = [\"baptism\", \"messiah\", \"catholicism\", \"resurrection\", \"christianity\", \"salvation\", \"protestant\", \"gospel\", \"trinity\", \"jesus\", \"christ\", \"christian\", \"cross\", \"catholic\", \"church\"]\n",
        "\n",
        "white_names = [\"harris\", \"nelson\", \"robinson\", \"thompson\", \"moore\", \"wright\", \"anderson\", \"clark\", \"jackson\", \"taylor\", \"scott\", \"davis\", \"allen\", \"adams\", \"lewis\", \"williams\", \"jones\", \"wilson\", \"martin\", \"johnson\"]\n",
        "hispanic_names= [\"ruiz\", \"alvarez\", \"vargas\", \"castillo\", \"gomez\", \"soto\", \"gonzalez\", \"sanchez\", \"rivera\", \"mendoza\", \"martinez\", \"torres\", \"rodriguez\", \"perez\", \"lopez\", \"medina\", \"diaz\", \"garcia\", \"castro\", \"cruz\"]\n"
      ],
      "metadata": {
        "id": "MMfzkzelfpbH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Call GetTopMostBiasedWords to obtain a list of the topk words with POS = ['JJ','JJR','JJS'] \n",
        "most biased towards women and men target sets in the model.\n",
        "\n",
        "The function returns two word lists, b1 and b2, which contain all words from the embedding model most biased towards\n",
        "women (b1) and men (b2). \n",
        "'''\n",
        "\n",
        "modelpath = 'Models'  #add your model here!\n",
        "[b1,b2] = GetTopMostBiasedWords(\n",
        "        modelpath,\n",
        "        300,\n",
        "        women,\n",
        "        men,\n",
        "        ['JJ','JJR','JJS'],\n",
        "        True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv-gqQpwfsKZ",
        "outputId": "6e2df306-e57d-4df5-a94f-ad1ef7650cc8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...0...100...200...300...400...500...600...700...800...900"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "List all topk biased words\n",
        "'''\n",
        "print('biased towards ', women)\n",
        "print( [b['word'] for b in b1[:30]] )\n",
        "print('biased towards ', men)\n",
        "print( [b['word'] for b in b2[:30]] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJewkwg-fw4l",
        "outputId": "b3d7a901-a85c-436d-a427-37bc14b9d215"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biased towards  ['sister', 'female', 'woman', 'girl', 'daughter', 'she', 'hers', 'her']\n",
            "['mutual', 'available', 'inexpensive', 'second', 'unplanned', 'formal', 'common', 'laughable', 'single', 'polish', 'innocuous', 'continued', 'local', 'chic', 'ethnic', 'neutral', 'genetic', 'viable', 'compatible', 'informal', 'okcupid', 'small', 'tangible', 'suitable', 'highest', 'enjoyable', 'third', 'specific', 'variable', 'probable']\n",
            "biased towards  ['brother', 'male', 'man', 'boy', 'son', 'he', 'his', 'him']\n",
            "['homosexual', 'ouch', 'unfriended', 'glorious', 'unapologetic', 'lest', 'respectable', 'underwear', 'mechanical', 'psychotic', 'overall', 'lustful', 'miserable', 'total', 'neurotic', 'metaphorical', 'enable', 'typical', 'ecstatic', 'obnoxious', 'dependable', 'inexperienced', 'sophisticated', 'stupid', 'ludicrous', 'delighted', 'pathetic', 'hippy', 'nuclear', 'honorable']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "Every word returned by GetTopMostBiasedWords contains the next attributes:\n",
        "word : Word \n",
        "bias : Bias strength towards target set 1 (in this example) when compared to target set 2\n",
        "freq : Frequency of word in the vocabulary of the model\n",
        "pos  : Part of speech as determined by NLTK\n",
        "wv   : Embedding of the word, used for clustering later\n",
        "rank : Frequency ranking of the word in model's vocabulary\n",
        "sent : Sentiment of word [-1,1], as determined by nltk.sentiment.vader\n",
        "\n",
        "Here we show the firt word biased towards women in the toy dataset\n",
        "'''\n",
        "b1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZqlxGNafzqC",
        "outputId": "36a0ca75-472f-496e-9a5b-51d5d43b3d4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bias': 0.16688398267389626,\n",
              " 'freq': 1678,\n",
              " 'pos': 'JJ',\n",
              " 'rank': 715,\n",
              " 'sent': 0.0,\n",
              " 'word': 'mutual',\n",
              " 'wv': array([-0.35153508,  2.4677126 ,  1.0783716 ,  1.9673408 ,  1.7866459 ,\n",
              "        -0.5691257 ,  2.5474048 ,  2.5896962 ,  0.00767454,  2.3252208 ,\n",
              "        -1.1459427 ,  1.0371476 ,  0.20932662, -1.2491125 , -1.6734018 ,\n",
              "        -2.2402112 ,  0.63953424,  1.8222603 ,  0.76552784,  0.5116544 ,\n",
              "        -1.7834792 ,  0.993913  , -1.4597697 ,  1.3452007 ,  0.2182632 ,\n",
              "         0.20111796, -0.8610432 , -0.81632304,  0.27136183,  0.96343505,\n",
              "        -1.3894559 ,  0.05518913,  1.5285295 ,  0.9776253 , -0.38843372,\n",
              "        -2.425641  , -0.8800297 , -0.39002603, -1.0597271 ,  0.337895  ,\n",
              "        -1.1285799 ,  0.89519095, -1.7965282 , -0.8947466 , -1.5264554 ,\n",
              "        -0.34988675, -1.8221968 , -0.9596129 , -1.2337615 ,  0.6578687 ,\n",
              "        -0.7387683 ,  0.10865816,  0.9348661 ,  0.1473172 , -0.2946774 ,\n",
              "         0.9619846 ,  3.7789378 ,  0.3296489 ,  0.9853502 , -0.72702414,\n",
              "         1.5374433 , -1.8208234 ,  0.29742545, -0.83744633, -0.59324443,\n",
              "         0.10048298,  0.09774517, -0.26572898,  0.49323925, -1.433416  ,\n",
              "         1.4454099 , -1.3447396 , -1.2880423 , -0.04711761,  1.0181177 ,\n",
              "         0.48477104,  0.07934887, -1.027812  , -0.30036873, -0.79280937,\n",
              "        -2.1050398 , -0.17817518,  1.2467831 ,  0.9505877 , -0.02528182,\n",
              "        -2.174415  , -0.18803187, -1.014329  , -3.337156  , -0.3954464 ,\n",
              "        -0.93722624,  0.9375161 , -2.4021697 ,  0.4476777 ,  2.0383148 ,\n",
              "         0.05060124,  1.689967  ,  0.24747518,  2.6608424 , -1.186704  ,\n",
              "         1.8709255 ,  0.38839594, -0.41843066,  0.2014959 , -1.5510491 ,\n",
              "         0.42111903,  0.19426546,  0.78405523,  1.0304625 , -1.0858006 ,\n",
              "         0.11891787, -0.83106506,  0.953468  , -2.6312742 , -0.85765326,\n",
              "        -0.22733521,  1.7417182 , -0.15076533, -0.00881085, -1.6820655 ,\n",
              "        -0.5248163 ,  0.6948109 , -1.5362116 ,  1.5152647 ,  1.0349315 ,\n",
              "        -0.09878648, -1.2109603 , -1.3635871 , -0.77954894, -1.2598842 ,\n",
              "        -1.2994125 ,  1.1477729 , -1.1290274 , -0.2777397 , -0.23488379,\n",
              "         1.9057752 , -0.58809227,  0.3282401 ,  0.9093073 ,  1.7864211 ,\n",
              "         0.59418976, -0.9972886 ,  0.46711385, -0.7706276 , -1.403721  ,\n",
              "        -1.4160117 , -2.2865863 , -1.4972086 ,  1.3159941 , -0.640315  ,\n",
              "        -1.8546774 , -0.49594542, -1.3871561 ,  1.5448228 , -0.5015076 ,\n",
              "        -1.8721603 , -1.6939988 , -1.0888898 ,  1.8944073 ,  0.43676248,\n",
              "         0.2158018 ,  0.07430657, -0.94105697, -0.8101846 ,  0.14482303,\n",
              "         0.05975702,  1.6747351 , -0.55480546,  0.5197589 ,  0.803587  ,\n",
              "         1.293865  , -1.3686277 ,  0.42460635,  0.35944325,  1.7755845 ,\n",
              "        -0.03921114,  0.6419876 , -0.18077861,  0.21627824, -1.539512  ,\n",
              "        -1.735395  , -0.43557677, -1.128647  ,  1.9153508 ,  0.45947385,\n",
              "        -1.2361102 , -1.1287426 , -1.2505974 ,  3.4454262 ,  0.3096457 ,\n",
              "        -1.7076529 , -0.20336331, -0.01623146, -0.04081471, -1.5949367 ,\n",
              "        -1.2085956 ,  1.4887253 ,  0.6231331 ,  0.44692767,  2.53283   ],\n",
              "       dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Here we show the firt word biased towards men in the toy dataset\n",
        "'''\n",
        "b2[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYVmCIPif1if",
        "outputId": "e3dcd385-36ee-4ee3-d796-87cd37d8627e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bias': 0.17531354396275067,\n",
              " 'freq': 43,\n",
              " 'pos': 'JJS',\n",
              " 'rank': 6389,\n",
              " 'sent': 0.0,\n",
              " 'word': 'lest',\n",
              " 'wv': array([ 0.41634107, -0.44387686, -0.00985347, -0.21305421,  0.5504241 ,\n",
              "        -0.18487473,  0.2682296 ,  0.49028108,  0.1429584 , -0.52414817,\n",
              "         0.07344105,  0.34229323,  0.12309747, -0.01019612, -0.3162536 ,\n",
              "        -0.26175988, -0.01533773,  0.07527173, -0.2821663 , -0.470527  ,\n",
              "         0.06452382,  0.18299761,  0.1139118 ,  0.72259825, -0.10858902,\n",
              "         0.26592755,  0.38285235,  0.09760041, -0.28722137, -0.11048467,\n",
              "        -0.06294207,  0.09396516, -0.16292803,  0.40241387,  0.0678149 ,\n",
              "        -0.02434599, -0.06206103, -0.4851445 , -0.58348846, -0.24393375,\n",
              "         0.06535346, -0.20651348, -0.13795072,  0.17772835, -0.22885354,\n",
              "        -0.12020966, -0.15249254,  0.03381014,  0.15814622, -0.45963234,\n",
              "        -0.43462208, -0.07656372,  0.16956717, -0.0106379 , -0.6811642 ,\n",
              "         0.36787388,  0.0735056 ,  0.34158555,  0.5385481 ,  0.3713525 ,\n",
              "         0.4059501 ,  0.11967969, -0.1393427 ,  0.8912286 , -0.09667005,\n",
              "         0.2502054 ,  0.13901499,  0.41737852, -0.09998218,  0.13053794,\n",
              "        -0.10707376,  0.37116212,  0.06468815,  0.13778564,  0.01147221,\n",
              "        -0.12484524,  0.21230537,  0.09371541, -0.56615585,  0.24872927,\n",
              "         0.00194015, -0.27207172, -0.16442576,  0.10515404,  0.10418525,\n",
              "        -0.01073297, -0.0021825 ,  0.04133472,  0.5436252 , -0.21037114,\n",
              "         0.07342932, -0.06422008, -0.19173992, -0.38509566,  0.03180292,\n",
              "        -0.05629619, -0.10486168,  0.40766394, -0.5224934 ,  0.2945888 ,\n",
              "         0.11063222, -0.40273184, -0.2776168 ,  0.09720101,  0.46729076,\n",
              "        -0.3217424 , -0.1252715 , -0.18767802, -0.14642747, -0.037324  ,\n",
              "         0.04943594, -0.01482561,  0.7134873 ,  0.22314614, -0.27268282,\n",
              "        -0.41957292,  0.34365883,  0.24214786, -0.01678619,  0.7211398 ,\n",
              "         0.05610364, -0.11520809,  0.16745766,  0.5576905 , -0.02084085,\n",
              "        -0.5820448 , -0.13130301,  0.37538084, -0.22483024,  0.13485049,\n",
              "         0.13444631,  0.36571655,  0.01314636,  0.11661518, -0.5493682 ,\n",
              "        -0.08762394,  0.04025729,  0.12657386, -0.47272286,  0.0694494 ,\n",
              "         0.30581257, -0.04709516,  0.14814973,  0.31497207,  0.38059494,\n",
              "        -0.25745726,  0.18240829,  0.34655446,  0.12077077, -0.0044854 ,\n",
              "        -0.32702366,  0.19103186,  0.04092754,  0.08931684,  0.31800523,\n",
              "        -0.06276561,  0.6873795 , -0.802666  , -0.18547833,  0.21011251,\n",
              "         0.17070617,  0.05312883,  0.20472233, -0.040996  , -0.13185436,\n",
              "        -0.14336483, -0.25379756, -0.10718233,  0.36290473,  0.02034329,\n",
              "        -0.3440205 ,  0.13772278, -0.12790617, -0.1816114 , -0.1026257 ,\n",
              "         0.2772932 , -0.17587818, -0.13150346, -0.38078427,  0.34513327,\n",
              "         0.12720844, -0.10365449,  0.54042417, -0.0199361 ,  0.20166203,\n",
              "         0.07606423, -0.04527636,  0.5073258 ,  0.33173358,  0.5875739 ,\n",
              "        -0.34805387, -0.109602  , -0.13078274, -0.19338758,  0.23389758,\n",
              "         0.0400384 ,  0.5810027 , -0.22290495, -0.00964184,  0.12299061],\n",
              "       dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Cluster words into concepts by leveragin their embedding distributions, where\n",
        "b1 : list of biased words towards target set 1\n",
        "b2 : list of biased words towards target set 2\n",
        "r  : r parameter for k-means clustering, where k = r*len(b)\n",
        "100: partition repetitoins for k-means, keeping the partition with best intrasim\n",
        "\n",
        "The function returns:\n",
        "List of clusters in a partition and words clustered in each cluster, for both target sets (cl1, cl2).\n",
        "'''\n",
        "[cl1,cl2] = Cluster(b1,b2, 0.15, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sExnfNY7f4X0",
        "outputId": "b77d3343-b88d-45c8-c726-fb8ab3166a7f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New partition for ts1, intrasim:  0.18748823020857017\n",
            "New partition for ts2, intrasim:  0.1089224949232526\n",
            "New partition for ts1, intrasim:  0.1652583090662702\n",
            "New partition for ts2, intrasim:  0.13907001723510964\n",
            "New partition for ts1, intrasim:  0.15556529131303576\n",
            "New partition for ts2, intrasim:  0.13631719497399303\n",
            "New partition for ts1, intrasim:  0.1646673558580657\n",
            "New partition for ts2, intrasim:  0.1211439098790006\n",
            "New partition for ts1, intrasim:  0.17705168786452222\n",
            "New partition for ts2, intrasim:  0.13905212268457431\n",
            "New partition for ts1, intrasim:  0.15846734465658824\n",
            "New partition for ts2, intrasim:  0.10756914874806522\n",
            "New partition for ts1, intrasim:  0.15938932687253232\n",
            "New partition for ts2, intrasim:  0.13175428691533567\n",
            "New partition for ts1, intrasim:  0.15481209208020122\n",
            "New partition for ts2, intrasim:  0.10627994930023228\n",
            "New partition for ts1, intrasim:  0.1715843015137376\n",
            "New partition for ts2, intrasim:  0.08829359400141044\n",
            "New partition for ts1, intrasim:  0.18261535974842802\n",
            "New partition for ts2, intrasim:  0.11997214065145181\n",
            "New partition for ts1, intrasim:  0.17547376445149324\n",
            "New partition for ts2, intrasim:  0.09750712063638302\n",
            "New partition for ts1, intrasim:  0.1401827579925029\n",
            "New partition for ts2, intrasim:  0.11510786862845442\n",
            "New partition for ts1, intrasim:  0.16969549273539802\n",
            "New partition for ts2, intrasim:  0.1372108376710219\n",
            "New partition for ts1, intrasim:  0.14417566034531432\n",
            "New partition for ts2, intrasim:  0.1346311785168934\n",
            "New partition for ts1, intrasim:  0.1719625775888193\n",
            "New partition for ts2, intrasim:  0.1493372499686292\n",
            "New partition for ts1, intrasim:  0.13735446574032736\n",
            "New partition for ts2, intrasim:  0.10739544730963979\n",
            "New partition for ts1, intrasim:  0.140019636692692\n",
            "New partition for ts2, intrasim:  0.13353912164326123\n",
            "New partition for ts1, intrasim:  0.1406805920637642\n",
            "New partition for ts2, intrasim:  0.14116476243655832\n",
            "New partition for ts1, intrasim:  0.1771506274271809\n",
            "New partition for ts2, intrasim:  0.10720541570463475\n",
            "New partition for ts1, intrasim:  0.1871123653181844\n",
            "New partition for ts2, intrasim:  0.16552871011629938\n",
            "New partition for ts1, intrasim:  0.2223648399740299\n",
            "New partition for ts2, intrasim:  0.08973488424264485\n",
            "New partition for ts1, intrasim:  0.1474652121465323\n",
            "New partition for ts2, intrasim:  0.08509131766799546\n",
            "New partition for ts1, intrasim:  0.15854698896364178\n",
            "New partition for ts2, intrasim:  0.11748538535591345\n",
            "New partition for ts1, intrasim:  0.16371232794703652\n",
            "New partition for ts2, intrasim:  0.12594651326647732\n",
            "New partition for ts1, intrasim:  0.14580198255019708\n",
            "New partition for ts2, intrasim:  0.0889023966161064\n",
            "New partition for ts1, intrasim:  0.17145578050131563\n",
            "New partition for ts2, intrasim:  0.12817648491899378\n",
            "New partition for ts1, intrasim:  0.1776774674129123\n",
            "New partition for ts2, intrasim:  0.11364072922032231\n",
            "New partition for ts1, intrasim:  0.1857060203542844\n",
            "New partition for ts2, intrasim:  0.1594097360592739\n",
            "New partition for ts1, intrasim:  0.1707616261254879\n",
            "New partition for ts2, intrasim:  0.09602064169155587\n",
            "New partition for ts1, intrasim:  0.16191457204839693\n",
            "New partition for ts2, intrasim:  0.11567675855164501\n",
            "New partition for ts1, intrasim:  0.15740996822856343\n",
            "New partition for ts2, intrasim:  0.1372033329217109\n",
            "New partition for ts1, intrasim:  0.16711170054327001\n",
            "New partition for ts2, intrasim:  0.10299581934310195\n",
            "New partition for ts1, intrasim:  0.15753308913602762\n",
            "New partition for ts2, intrasim:  0.13075847117585776\n",
            "New partition for ts1, intrasim:  0.14447842175517125\n",
            "New partition for ts2, intrasim:  0.12121349213794486\n",
            "New partition for ts1, intrasim:  0.15772991968192304\n",
            "New partition for ts2, intrasim:  0.12779302819555152\n",
            "New partition for ts1, intrasim:  0.15249634178395535\n",
            "New partition for ts2, intrasim:  0.11958192683098479\n",
            "New partition for ts1, intrasim:  0.1924500918762904\n",
            "New partition for ts2, intrasim:  0.14395974062301445\n",
            "New partition for ts1, intrasim:  0.16178230971941343\n",
            "New partition for ts2, intrasim:  0.1370325858328158\n",
            "New partition for ts1, intrasim:  0.1673227920255454\n",
            "New partition for ts2, intrasim:  0.146731641115976\n",
            "New partition for ts1, intrasim:  0.1441443629255038\n",
            "New partition for ts2, intrasim:  0.1089370657320583\n",
            "New partition for ts1, intrasim:  0.15764711614547333\n",
            "New partition for ts2, intrasim:  0.13481220843249095\n",
            "New partition for ts1, intrasim:  0.13812457821114757\n",
            "New partition for ts2, intrasim:  0.07466241521829223\n",
            "New partition for ts1, intrasim:  0.17162364644414682\n",
            "New partition for ts2, intrasim:  0.11792298703145108\n",
            "New partition for ts1, intrasim:  0.19959319698516262\n",
            "New partition for ts2, intrasim:  0.12376845697033774\n",
            "New partition for ts1, intrasim:  0.15561748635454245\n",
            "New partition for ts2, intrasim:  0.1177301951574521\n",
            "New partition for ts1, intrasim:  0.16441200743441733\n",
            "New partition for ts2, intrasim:  0.16799544462080168\n",
            "New partition for ts1, intrasim:  0.15286710982844418\n",
            "New partition for ts2, intrasim:  0.10618074754963662\n",
            "New partition for ts1, intrasim:  0.13457405829603583\n",
            "New partition for ts2, intrasim:  0.12974748233390823\n",
            "New partition for ts1, intrasim:  0.17611054111296187\n",
            "New partition for ts2, intrasim:  0.11439277612885694\n",
            "New partition for ts1, intrasim:  0.17615736389892472\n",
            "New partition for ts2, intrasim:  0.14480329021728225\n",
            "New partition for ts1, intrasim:  0.1460696311095513\n",
            "New partition for ts2, intrasim:  0.10921780555032186\n",
            "New partition for ts1, intrasim:  0.15951132819313785\n",
            "New partition for ts2, intrasim:  0.14335247903132312\n",
            "New partition for ts1, intrasim:  0.16640363524238577\n",
            "New partition for ts2, intrasim:  0.11340991913158427\n",
            "New partition for ts1, intrasim:  0.1754563008775916\n",
            "New partition for ts2, intrasim:  0.14930406100432286\n",
            "New partition for ts1, intrasim:  0.19072883677849017\n",
            "New partition for ts2, intrasim:  0.11494333058764167\n",
            "New partition for ts1, intrasim:  0.1439358867451962\n",
            "New partition for ts2, intrasim:  0.12169528498292916\n",
            "New partition for ts1, intrasim:  0.12294629344684771\n",
            "New partition for ts2, intrasim:  0.142404611114542\n",
            "New partition for ts1, intrasim:  0.17174419079081205\n",
            "New partition for ts2, intrasim:  0.1150384509200449\n",
            "New partition for ts1, intrasim:  0.18489528732582425\n",
            "New partition for ts2, intrasim:  0.14404342750753732\n",
            "New partition for ts1, intrasim:  0.19369148365317584\n",
            "New partition for ts2, intrasim:  0.15080228574594667\n",
            "New partition for ts1, intrasim:  0.1889287351008887\n",
            "New partition for ts2, intrasim:  0.13659474751101452\n",
            "New partition for ts1, intrasim:  0.17345835781756944\n",
            "New partition for ts2, intrasim:  0.11040167678200404\n",
            "New partition for ts1, intrasim:  0.18358044537942042\n",
            "New partition for ts2, intrasim:  0.1277997606117185\n",
            "New partition for ts1, intrasim:  0.14364255265380832\n",
            "New partition for ts2, intrasim:  0.15221546673891273\n",
            "New partition for ts1, intrasim:  0.1742186031680661\n",
            "New partition for ts2, intrasim:  0.13570309702808112\n",
            "New partition for ts1, intrasim:  0.17624824524891303\n",
            "New partition for ts2, intrasim:  0.1120630660264819\n",
            "New partition for ts1, intrasim:  0.18431040056099693\n",
            "New partition for ts2, intrasim:  0.15047123875325322\n",
            "New partition for ts1, intrasim:  0.1539005925553399\n",
            "New partition for ts2, intrasim:  0.11883725858918065\n",
            "New partition for ts1, intrasim:  0.14054156602239365\n",
            "New partition for ts2, intrasim:  0.11678514577513663\n",
            "New partition for ts1, intrasim:  0.15241669948859657\n",
            "New partition for ts2, intrasim:  0.10296684261408785\n",
            "New partition for ts1, intrasim:  0.18763925644479273\n",
            "New partition for ts2, intrasim:  0.13162772623086952\n",
            "New partition for ts1, intrasim:  0.1386565777187372\n",
            "New partition for ts2, intrasim:  0.12417470357456414\n",
            "New partition for ts1, intrasim:  0.15605426491905383\n",
            "New partition for ts2, intrasim:  0.09877969513697817\n",
            "New partition for ts1, intrasim:  0.13739571429708394\n",
            "New partition for ts2, intrasim:  0.1373284798805353\n",
            "New partition for ts1, intrasim:  0.13499105797866653\n",
            "New partition for ts2, intrasim:  0.1367485028238226\n",
            "New partition for ts1, intrasim:  0.14829356429444268\n",
            "New partition for ts2, intrasim:  0.14561699047535928\n",
            "New partition for ts1, intrasim:  0.15211720586496133\n",
            "New partition for ts2, intrasim:  0.11950703276502404\n",
            "New partition for ts1, intrasim:  0.16512753819677126\n",
            "New partition for ts2, intrasim:  0.09225154463974942\n",
            "New partition for ts1, intrasim:  0.17019063639143672\n",
            "New partition for ts2, intrasim:  0.10285313091994384\n",
            "New partition for ts1, intrasim:  0.17940505098418036\n",
            "New partition for ts2, intrasim:  0.13589657342166112\n",
            "New partition for ts1, intrasim:  0.13443848805631123\n",
            "New partition for ts2, intrasim:  0.09430510741696556\n",
            "New partition for ts1, intrasim:  0.18734669345033308\n",
            "New partition for ts2, intrasim:  0.15121546875573944\n",
            "New partition for ts1, intrasim:  0.1622015968675471\n",
            "New partition for ts2, intrasim:  0.10414208934145909\n",
            "New partition for ts1, intrasim:  0.1814115714621289\n",
            "New partition for ts2, intrasim:  0.1520930814361894\n",
            "New partition for ts1, intrasim:  0.16659017617816607\n",
            "New partition for ts2, intrasim:  0.1156441146167546\n",
            "New partition for ts1, intrasim:  0.14422530869117686\n",
            "New partition for ts2, intrasim:  0.09288700689731644\n",
            "New partition for ts1, intrasim:  0.15909445024909935\n",
            "New partition for ts2, intrasim:  0.12190603769459156\n",
            "New partition for ts1, intrasim:  0.1544479390608987\n",
            "New partition for ts2, intrasim:  0.14216808589568247\n",
            "New partition for ts1, intrasim:  0.1802433763129529\n",
            "New partition for ts2, intrasim:  0.13623617581101424\n",
            "New partition for ts1, intrasim:  0.1503729077498845\n",
            "New partition for ts2, intrasim:  0.13550159842128245\n",
            "New partition for ts1, intrasim:  0.1753927162215151\n",
            "New partition for ts2, intrasim:  0.09174259742788536\n",
            "New partition for ts1, intrasim:  0.17939632137373032\n",
            "New partition for ts2, intrasim:  0.13755791984917345\n",
            "New partition for ts1, intrasim:  0.147686577273729\n",
            "New partition for ts2, intrasim:  0.14520836701497103\n",
            "New partition for ts1, intrasim:  0.19798006191711665\n",
            "New partition for ts2, intrasim:  0.13220896070433028\n",
            "New partition for ts1, intrasim:  0.16547530898231952\n",
            "New partition for ts2, intrasim:  0.13036110204365148\n",
            "New partition for ts1, intrasim:  0.1133293323057253\n",
            "New partition for ts2, intrasim:  0.1271224809760896\n",
            "New partition for ts1, intrasim:  0.14677498496291455\n",
            "New partition for ts2, intrasim:  0.14246557370372712\n",
            "New partition for ts1, intrasim:  0.186750425392291\n",
            "New partition for ts2, intrasim:  0.143464987686224\n",
            "New partition for ts1, intrasim:  0.16875101661962022\n",
            "New partition for ts2, intrasim:  0.124992564881565\n",
            "New partition for ts1, intrasim:  0.1607226258959908\n",
            "New partition for ts2, intrasim:  0.13614461545340925\n",
            "[*] Intrasim of best partition found for ts1,  0.2223648399740299\n",
            "[*] Intrasim of best partition found for ts2,  0.16799544462080168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Exploring the conceptual biases from partition biased towards ts1, only printing the words in each cluster.\n",
        "'''\n",
        "#conceptual biases for target set 1\n",
        "print(len(cl1))\n",
        "for cluster in cl1:\n",
        "    print( [k['word'] for k in cluster] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh2-zNg0f7_t",
        "outputId": "37020b41-01f9-4101-b6f2-d017bb18a9fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "['local', 'wide', 'italian', 'uni', 'urban', 'slower', 'musical', 'foreign', 'vegetarian', 'new', 'green', 'fresh']\n",
            "['ready']\n",
            "['easier']\n",
            "['common']\n",
            "['neutral', 'viable', 'suitable', 'probable', 'intentional', 'plausible', 'broad', 'significant', 'familiar', 'preferable', 'impersonal', 'unreliable', 'invasive', 'unusual', 'applicable', 'beneficial', 'definitive', 'productive', 'false', 'modest', 'disappointed', 'unnatural', 'knowledgeable', 'unable', 'unapproachable', 'impressive', 'risky', 'illegal', 'unimportant', 'satisfied', 'rigid', 'unwilling', 'accepted', 'humorous', 'vocal', 'unclear', 'questionable', 'offensive', 'trivial']\n",
            "['current', 'dynamic', 'cultural', 'political', 'potential', 'various', 'moral', 'religious', 'asian']\n",
            "['difficult', 'hard']\n",
            "['exclusive']\n",
            "['comfortable', 'uncomfortable']\n",
            "['similar']\n",
            "['interested']\n",
            "['few', 'several']\n",
            "['enjoyable', 'specific', 'enthusiastic', 'romantic', 'creative', 'approachable', 'adventurous', 'memorable', 'private', 'flirtatious', 'serious', 'heavy', 'effective', 'quiet']\n",
            "['desirable', 'attractive']\n",
            "['compatible', 'different', 'incompatible']\n",
            "['single']\n",
            "['free']\n",
            "['negative', 'positive']\n",
            "['small']\n",
            "['lower']\n",
            "['much']\n",
            "['least']\n",
            "['okcupid']\n",
            "['open']\n",
            "['good', 'great']\n",
            "['innocuous', 'informal', 'individual', 'unexpected', 'organic', 'actual', 'international', 'ethical', 'exotic', 'arbitrary', 'interactive', 'upper', 'external', 'ordinary', 'unfamiliar', 'additional', 'integral', 'unspoken', 'automatic', 'intuitive', 'unnecessary', 'intellectual', 'indian', 'equal', 'academic', 'unknown', 'autonomous', 'excessive', 'abnormal', 'ulterior', 'eventual', 'unequal']\n",
            "['acceptable']\n",
            "['other']\n",
            "['ive']\n",
            "['easy']\n",
            "['higher', 'greater']\n",
            "['smaller', 'large', 'larger']\n",
            "['available', 'busy']\n",
            "['second', 'third', 'fourth']\n",
            "['clear', 'obvious']\n",
            "['extra']\n",
            "['initial']\n",
            "['special']\n",
            "['personal']\n",
            "['formal', 'casual', 'expensive']\n",
            "['mutual', 'inexpensive', 'unplanned', 'laughable', 'polish', 'continued', 'chic', 'ethnic', 'genetic', 'tangible', 'highest', 'variable', 'geographical', 'arrive', 'crumble', 'unofficial', 'olive', 'central', 'instantaneous', 'visible', 'uphold', 'virtual', 'spanish', 'environmental', 'agreeable', 'achievable', 'recreational', 'monetary', 'manual', 'narrative', 'fumble', 'ritual', 'preliminary', 'delicious', 'lavish', 'racial', 'weakest', 'doable', 'earliest', 'unambiguous', 'botanical', 'iced', 'affordable', 'nonsexual', 'logistical', 'unattached', 'adjective', 'unannounced', 'unresponsive', 'dietary', 'final', 'controllable', 'particular', 'interracial', 'fabulous', 'justifiable', 'unheard', 'scan', 'advantageous', 'frivolous', 'unnoticed', 'senior', 'ineffective', 'negotiable', 'gradual', 'undecided', 'immaterial', 'golden', 'vocabulary', 'rapid', 'mexican', 'relatable', 'unlimited', 'national', 'economic', 'youngest', 'facial', 'comparable', 'stumble', 'controversial', 'noncommittal', 'continuous', 'optimal', 'upscale', 'historical', 'residual', 'solitary', 'tiny', 'conventional', 'subconscious', 'hypothetical', 'aesthetic', 'amenable', 'fewer', 'noticable', 'undivided', 'accessible', 'artistic', 'immoral', 'unwarranted', 'geographic', 'foreseeable', 'unlocked', 'substantive', 'latest', 'ramble', 'diplomatic', 'nonverbal', 'irish', 'therapeutic', 'magnetic', 'transsexual', 'believable', 'prospective', 'advisable', 'unprepared', 'inseparable', 'unfulfilled', 'personable', 'fullest', 'sic', 'domestic', 'noble', 'usual', 'contagious', 'manageable']\n",
            "['physical']\n",
            "['live']\n",
            "['low']\n",
            "['possible', 'crucial', 'unlikely', 'limited', 'basic', 'useful', 'necessary', 'normal', 'reasonable', 'important', 'impossible', 'understandable', 'solid']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Exploring the conceptual biases from partition biased towards ts2, only printing the words in each cluster.\n",
        "'''\n",
        "print(len(cl2))\n",
        "for cluster in cl2:\n",
        "  print( [k['word'] for k in cluster] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku4p2fh6f-q7",
        "outputId": "68311d69-6215-4e33-910c-c099dd33d057"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "['homosexual', 'ouch', 'unfriended', 'glorious', 'unapologetic', 'lest', 'underwear', 'mechanical', 'psychotic', 'lustful', 'metaphorical', 'enable', 'ecstatic', 'dependable', 'sophisticated', 'ludicrous', 'delighted', 'hippy', 'nuclear', 'bearable', 'tic', 'literary', 'vicious', 'unlovable', 'uncalled', 'argumentative', 'preppy', 'sympathetic', 'notable', 'unkempt', 'enigmatic', 'unfaithful', 'antisocial', 'idiotic', 'electric', 'unexperienced', 'vietnamese', 'furious', 'pompous', 'asynchronous', 'induced', 'devious', 'envious', 'unemotional', 'overdrive', 'pedantic', 'horrendous', 'infamous', 'injured', 'wisest', 'freudian', 'facetious', 'courageous', 'swedish', 'improbable', 'interchangeable', 'psychic', 'invisible', 'optional', 'largest', 'plural', 'mathematical', 'extraordinary', 'unproductive', 'likable', 'unmotivated', 'urbandictionary', 'irritable', 'irresistible', 'unworthy', 'tactical', 'observational', 'offish', 'unfavorable', 'steady', 'inquisitive', 'corporate', 'unsatisfied', 'nonsensical', 'fixable', 'egotistical', 'unmarried', 'loose', 'agressive', 'perpetual', 'pragmatic', 'philosophical', 'callous', 'radical', 'nigh', 'gullible', 'sociopathic', 'ethic', 'gregarious', 'concious', 'customary', 'criminal', 'unconventional', 'salvageable', 'theoretical', 'despicable', 'untrustworthy', 'quickest', 'melodramatic', 'demisexual', 'archive', 'grammatical', 'elusive', 'fashionable', 'behavioral', 'susceptible', 'chaotic', 'libertarian', 'eccentric', 'precious', 'unsolicited', 'formative', 'upvoted', 'gigantic', 'predictable', 'mystical', 'accountable', 'slouch', 'regrettable', 'factual', 'bubble', 'liable', 'overdue', 'secondary', 'upright', 'simultaneous', 'overzealous', 'topical', 'diary', 'invalid', 'righteous', 'mythical', 'nest', 'pleasurable', 'fictional', 'unprotected', 'unpredictable', 'scrabble', 'tremendous', 'disastrous', 'sloppy', 'unanimous', 'traumatic', 'disposable', 'poisonous', 'classical', 'educational', 'manifest', 'pleased', 'specialized', 'commercial']\n",
            "['worried']\n",
            "['same']\n",
            "['rich', 'black', 'hot']\n",
            "['respectable', 'powerful', 'sensitive', 'mysterious', 'humble', 'charismatic', 'sensible', 'skeptical', 'competitive', 'uptight', 'civil', 'generous', 'complex', 'soft', 'expressive', 'conservative', 'promiscuous', 'chivalrous', 'empathetic', 'ambitious', 'attentive', 'impressed', 'practical']\n",
            "['entire', 'whole']\n",
            "['oblivious', 'uninterested', 'unattractive']\n",
            "['experienced']\n",
            "['aware']\n",
            "['sudden']\n",
            "['pessimistic', 'cynical', 'dramatic', 'sarcastic', 'realistic', 'ambiguous', 'critical']\n",
            "['huge']\n",
            "['responsible']\n",
            "['naked']\n",
            "['due']\n",
            "['direct']\n",
            "['horrible', 'terrible']\n",
            "['guilty']\n",
            "['stable']\n",
            "['many']\n",
            "['old']\n",
            "['overall', 'honorable', 'massive', 'heterosexual', 'ultra', 'bisexual', 'exceptional', 'biological', 'imaginary', 'unpopular', 'classic', 'stereotypical', 'famous', 'anonymous', 'obese', 'incapable', 'identical', 'hispanic', 'eastern', 'inevitable', 'modern', 'military', 'african', 'autistic', 'united', 'western', 'disabled', 'essential', 'southern', 'nicest', 'grand', 'comic']\n",
            "['total', 'complete']\n",
            "['assertive', 'aggressive']\n",
            "['full']\n",
            "['rid']\n",
            "['stupid']\n",
            "['such']\n",
            "['jealous']\n",
            "['abusive', 'vulnerable', 'unstable']\n",
            "['bigger']\n",
            "['sexual']\n",
            "['puppy']\n",
            "['american']\n",
            "['miserable', 'unhappy']\n",
            "['inexperienced', 'naive']\n",
            "['poor']\n",
            "['scary']\n",
            "['safe']\n",
            "['typical']\n",
            "['daily']\n",
            "['pathetic']\n",
            "['depressed', 'anxious']\n",
            "['neurotic', 'obnoxious', 'obsessive', 'gross', 'apprehensive', 'abrasive', 'pretentious', 'arrogant', 'nasty', 'misogynistic', 'apologetic', 'materialistic', 'insensitive', 'unprofessional', 'justified', 'hypocritical', 'asexual', 'narcissistic', 'crappy', 'sappy', 'apathetic', 'alive', 'dangerous', 'indecisive', 'ridiculous', 'irresponsible', 'suspicious', 'irrational', 'undesirable']\n",
            "['forgive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "Reuse some codes from https://github.com/xfold/LanguageBiasesInReddit\n"
      ],
      "metadata": {
        "id": "azNAlpYCKqU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rZOyyRyBh8BU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}